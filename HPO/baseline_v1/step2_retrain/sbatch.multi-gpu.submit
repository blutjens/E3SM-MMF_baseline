#!/bin/bash
#SBATCH --job-name="Retrain_multi-nodes"
#SBATCH --output="logs/Task3-Step2-retrain.%j.%N.out"
#SBATCH -q regular
#SBATCH --nodes=2
#SBATCH --gpus=8
#SBATCH --constraint=gpu
#SBATCH --export=ALL
#SBATCH --account=m4331
#SBATCH --mail-type=ALL
#SBATCH --mail-user=sungduk@uci.edu
#SBATCH -t 12:00:00

module load parallel

# jobid
jobid=$SLURM_JOB_ID

# slurm progress log
f_progress=./parallel.progress.${jobid}.log

# working dir
export WDIR=`pwd`

# set number of jobs per node (note that 4 gpus per node)
export JOBS_PER_NODE=22

# set up gnu parallel
bash expand_nodelist.sh $SLURM_JOB_NODELIST > logs/hostfile.${jobid}.txt

parallel -j $JOBS_PER_NODE --delay 20 --joblog $f_progress --slf logs/hostfile.${jobid}.txt --wd /global/cfs/cdirs/m4331/sungduk/LEAP/E3SM-MMF_baseline/HPO/baseline_v1/step2_retrain "module load cudnn cudatoolkit; source ~/.bashrc; conda activate tf_conda; python step2_retrain.py" :::: top44.lot.txt ::::+ top44.trial.txt
# note that -j is for "jobs PER NODE"
# add --resume for continuing parallel job
